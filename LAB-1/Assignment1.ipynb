{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgdO5-PYgTOf",
        "outputId": "f5c36a55-0c50-46f0-c44c-059031e60043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ASSIGNMENT 1: Text Preprocessing with Hindi IndicCorpV2\n",
            "============================================================\n",
            "Dataset: ai4bharat/IndicCorpV2 (Hindi - hi-1.txt)\n",
            "Environment: Google Colab Optimized\n",
            "============================================================\n",
            "Loading dataset stream...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset stream loaded successfully!\n",
            "\n",
            "Initial memory usage: 555.2MB\n",
            "\n",
            "Processing Hindi dataset...\n",
            "\n",
            "SAMPLE PROCESSING (First 3 examples):\n",
            "----------------------------------------\n",
            "\n",
            "--- Example 1 ---\n",
            "Original Paragraph:\n",
            "‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§¨‡§ø‡§≤‡•ã‡§Ç ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§¶‡•á‡§®‡§æ ‡§π‡•Ä ‡§â‡§®‡§ï‡§æ ‡§ï‡§æ‡§Æ\n",
            "\n",
            "Sentences found: 1\n",
            "\n",
            "First sentence tokens:\n",
            "['‡§≤‡•ã‡§ó‡•ã‡§Ç', '‡§ï‡•ã', '‡§¨‡§ø‡§≤‡•ã‡§Ç', '‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä', '‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ', '‡§¶‡•á‡§®‡§æ', '‡§π‡•Ä', '‡§â‡§®‡§ï‡§æ', '‡§ï‡§æ‡§Æ']\n",
            "\n",
            "--- Example 2 ---\n",
            "Original Paragraph:\n",
            "‡§á‡§®‡•á‡§≤‡•ã 1987 ‡§Æ‡•á‡§Ç ‡§â‡§∏ ‡§µ‡§ï‡•ç‡§§ ‡§ê‡§∏‡•á ‡§π‡•Ä ‡§¶‡•ã‡§∞‡§æ‡§π‡•á ‡§™‡§∞ ‡§ñ‡§°‡§º‡•Ä ‡§•‡•Ä, ‡§ú‡§¨ ‡§™‡•Ç‡§∞‡•ç‡§µ ‡§â‡§™‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§¶‡•á‡§µ‡•Ä‡§≤‡§æ‡§≤ ‡§®‡•á ‡§Ö‡§™‡§®‡•á ‡§™‡•Å‡§§‡•ç‡§∞ ‡§ì‡§Æ‡§™‡•ç‡§∞‡§ï‡§æ‡§∂ ‡§ö‡•å‡§ü‡§æ‡§≤‡§æ ‡§ï‡•ã ‡§Ö‡§™‡§®‡§æ ‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ï ‡§â‡§§‡•ç‡§§‡§∞‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä ‡§ò‡•ã‡§∑‡§ø‡§§ ‡§ï‡§ø‡§Ø...\n",
            "\n",
            "Sentences found: 7\n",
            "\n",
            "First sentence tokens:\n",
            "['‡§á‡§®‡•á‡§≤‡•ã', '1987', '‡§Æ‡•á‡§Ç', '‡§â‡§∏', '‡§µ‡§ï‡•ç‡§§', '‡§ê‡§∏‡•á', '‡§π‡•Ä', '‡§¶‡•ã‡§∞‡§æ‡§π‡•á', '‡§™‡§∞', '‡§ñ‡§°‡§º‡•Ä']\n",
            "... and 18 more tokens\n",
            "\n",
            "--- Example 3 ---\n",
            "Original Paragraph:\n",
            "‡§ú‡§π‡§æ‡§Ç ‡§Ü‡§à ‡§•‡•Ä ‡§§‡§¨‡§æ‡§π‡•Ä ‡§â‡§∏ ‡§ò‡§æ‡§ü‡•Ä ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§ñ‡§§‡§∞‡§æ ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ\n",
            "\n",
            "Sentences found: 1\n",
            "\n",
            "First sentence tokens:\n",
            "['‡§ú‡§π‡§æ‡§Ç', '‡§Ü‡§à', '‡§•‡•Ä', '‡§§‡§¨‡§æ‡§π‡•Ä', '‡§â‡§∏', '‡§ò‡§æ‡§ü‡•Ä', '‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞', '‡§Æ‡•á‡§Ç', '‡§ñ‡§§‡§∞‡§æ', '‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ']\n",
            "\n",
            "üìä Progress: 1,000 documents processed\n",
            "   Sentences: 3,432\n",
            "   Words: 60,709\n",
            "   Memory: 682.2MB\n",
            "\n",
            "üìä Progress: 2,000 documents processed\n",
            "   Sentences: 6,901\n",
            "   Words: 119,426\n",
            "   Memory: 691.0MB\n",
            "\n",
            "üìä Progress: 3,000 documents processed\n",
            "   Sentences: 10,455\n",
            "   Words: 181,678\n",
            "   Memory: 700.5MB\n",
            "\n",
            "üìä Progress: 4,000 documents processed\n",
            "   Sentences: 13,914\n",
            "   Words: 240,895\n",
            "   Memory: 711.1MB\n",
            "\n",
            "üìä Progress: 5,000 documents processed\n",
            "   Sentences: 17,461\n",
            "   Words: 304,777\n",
            "   Memory: 719.4MB\n",
            "\n",
            "üìä Progress: 6,000 documents processed\n",
            "   Sentences: 20,816\n",
            "   Words: 362,421\n",
            "   Memory: 727.1MB\n",
            "\n",
            "üìä Progress: 7,000 documents processed\n",
            "   Sentences: 24,694\n",
            "   Words: 430,383\n",
            "   Memory: 735.9MB\n",
            "\n",
            "üìä Progress: 8,000 documents processed\n",
            "   Sentences: 28,257\n",
            "   Words: 493,184\n",
            "   Memory: 747.7MB\n",
            "\n",
            "üìä Progress: 9,000 documents processed\n",
            "   Sentences: 31,876\n",
            "   Words: 557,295\n",
            "   Memory: 756.0MB\n",
            "\n",
            "üìä Progress: 10,000 documents processed\n",
            "   Sentences: 35,391\n",
            "   Words: 617,616\n",
            "   Memory: 764.0MB\n",
            "\n",
            "üõë Reached processing limit of 10,000 documents\n",
            "\n",
            "‚úÖ Processing complete!\n",
            "üìä Processed 10,000 documents\n",
            "üíæ Current memory usage: 758.2MB\n",
            "\n",
            "üíæ Saving tokenized data and computing statistics...\n",
            "\n",
            "============================================================\n",
            "FINAL CORPUS STATISTICS (Assignment Task 1d)\n",
            "============================================================\n",
            "i.   Total number of sentences: 35,391\n",
            "ii.  Total number of words: 617,616\n",
            "iii. Total number of characters: 2,918,403\n",
            "iv.  Average Sentence Length: 17.45 words per sentence\n",
            "v.   Average word length: 3.84 characters per word\n",
            "vi.  Type/Token Ratio (TTR): 0.0665\n",
            "------------------------------------------------------------\n",
            "Vocabulary size: 41,043 unique tokens\n",
            "Processed documents: 10,000\n",
            "============================================================\n",
            "\n",
            "üéØ ASSIGNMENT TASK 1 COMPLETED!\n",
            "üìÅ Check 'hindi_corpus_output' directory for:\n",
            "   üìÑ tokenized_data_final.json - Tokenized documents\n",
            "   üìä corpus_statistics.json - All computed statistics\n",
            "   üìù statistics_report.txt - Detailed human-readable report\n",
            "   üî§ vocabulary_top1000.json - Most frequent words\n",
            "\n",
            "‚úÖ All requirements completed:\n",
            "   a. ‚úÖ Downloaded and processed 10,000 Hindi documents\n",
            "   b. ‚úÖ Tokenized 35,391 sentences into 617,616 words\n",
            "   c. ‚úÖ Saved all tokenized data\n",
            "   d. ‚úÖ Computed comprehensive corpus statistics\n",
            "\n",
            "üìà CORPUS INSIGHTS:\n",
            "   üî§ Vocabulary richness: 41,043 unique tokens\n",
            "   üìù Text density: 2,918,403 characters processed\n",
            "   üîÑ Language diversity: TTR = 0.0665\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "import unicodedata\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Any\n",
        "import gc\n",
        "import psutil\n",
        "import time\n",
        "\n",
        "# -------------------- Memory Management Helper --------------------\n",
        "def get_memory_usage():\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    try:\n",
        "        process = psutil.Process(os.getpid())\n",
        "        return process.memory_info().rss / 1024 / 1024\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Force garbage collection to free memory\"\"\"\n",
        "    gc.collect()\n",
        "\n",
        "# -------------------- Sentence Tokenizer --------------------\n",
        "def sentence_split(paragraph):\n",
        "    \"\"\"Split paragraph into sentences using Hindi and English punctuation\"\"\"\n",
        "    sentence_endings = ['.', '?', '!', '‡•§']\n",
        "    sentences = []\n",
        "    current = ''\n",
        "\n",
        "    for char in paragraph:\n",
        "        current += char\n",
        "        if char in sentence_endings:\n",
        "            if current.strip():\n",
        "                sentences.append(current.strip())\n",
        "                current = ''\n",
        "\n",
        "    if current.strip():\n",
        "        sentences.append(current.strip())\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# -------------------- Word Tokenizer using regex --------------------\n",
        "def word_tokenize(sentence):\n",
        "    \"\"\"Tokenize sentence into words handling URLs, emails, dates, numbers, Hindi and English text\"\"\"\n",
        "    pattern = r'''\n",
        "        (https?://[^\\s]+) |                     # Group 1: URLs\n",
        "        (www\\.[^\\s]+) |                         # Group 2: www URLs\n",
        "        (\\w+@\\w+\\.\\w+) |                        # Group 3: Emails\n",
        "        (\\d{1,2}/\\d{1,2}/\\d{2,4}) |             # Group 4: Dates\n",
        "        (\\d+\\.\\d+) |                            # Group 5: Decimal numbers\n",
        "        ([\\u0900-\\u097F]+) |                    # Group 6: Hindi (Devanagari) words\n",
        "        ([a-zA-Z0-9_-]+) |                      # Group 7: Latin words/digits\n",
        "        ([^\\s])                                 # Group 8: Other single chars\n",
        "    '''\n",
        "\n",
        "    tokens = re.findall(pattern, sentence, re.VERBOSE)\n",
        "    flat_tokens = [token for group in tokens for token in group if token]\n",
        "    return flat_tokens\n",
        "\n",
        "# -------------------- Corpus Statistics Class --------------------\n",
        "class CorpusStatistics:\n",
        "    def __init__(self):\n",
        "        self.total_sentences = 0\n",
        "        self.total_words = 0\n",
        "        self.total_characters = 0\n",
        "        self.sentence_lengths = []\n",
        "        self.word_lengths = []\n",
        "        self.vocabulary = Counter()\n",
        "        self.processed_documents = 0\n",
        "        self.tokenized_data = []\n",
        "        self.memory_threshold = 1000  # MB - adjust based on Colab limits\n",
        "\n",
        "    def check_memory_and_save(self, output_dir: str = \"hindi_corpus_output\"):\n",
        "        \"\"\"Check memory usage and save checkpoint if needed\"\"\"\n",
        "        current_memory = get_memory_usage()\n",
        "\n",
        "        if current_memory > self.memory_threshold:\n",
        "            print(f\"\\nMemory usage: {current_memory:.1f}MB - Saving checkpoint...\")\n",
        "\n",
        "            # Save current data\n",
        "            self.save_checkpoint(output_dir)\n",
        "\n",
        "            # Clear tokenized data to free memory but keep statistics\n",
        "            self.tokenized_data = []\n",
        "            clear_memory()\n",
        "\n",
        "            print(f\"Memory cleared. Current usage: {get_memory_usage():.1f}MB\")\n",
        "\n",
        "    def process_document(self, text: str, doc_id: int) -> Dict[str, Any]:\n",
        "        \"\"\"Process a single document and update statistics\"\"\"\n",
        "        if not text or not text.strip():\n",
        "            return None\n",
        "\n",
        "        text = text.strip()\n",
        "\n",
        "        # Skip extremely long documents to prevent memory issues\n",
        "        if len(text) > 50000:  # Skip documents longer than 50K characters\n",
        "            return None\n",
        "\n",
        "        sentences = sentence_split(text)\n",
        "        processed_sentences = []\n",
        "        doc_word_count = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if not sentence.strip():\n",
        "                continue\n",
        "\n",
        "            words = word_tokenize(sentence)\n",
        "\n",
        "            if words:  # Only process non-empty sentences\n",
        "                processed_sentences.append({\n",
        "                    'text': sentence,\n",
        "                    'tokens': words,\n",
        "                    'word_count': len(words)\n",
        "                })\n",
        "\n",
        "                # Update statistics\n",
        "                self.total_sentences += 1\n",
        "                self.total_words += len(words)\n",
        "                self.total_characters += len(sentence)\n",
        "                self.sentence_lengths.append(len(words))\n",
        "\n",
        "                # Update vocabulary\n",
        "                self.vocabulary.update(words)\n",
        "\n",
        "                # Track word lengths\n",
        "                for word in words:\n",
        "                    self.word_lengths.append(len(word))\n",
        "\n",
        "                doc_word_count += len(words)\n",
        "\n",
        "        if processed_sentences:\n",
        "            self.processed_documents += 1\n",
        "\n",
        "            document_data = {\n",
        "                'document_id': doc_id,\n",
        "                'original_text': text[:1000] + \"...\" if len(text) > 1000 else text,  # Truncate for memory\n",
        "                'sentences': processed_sentences,\n",
        "                'document_stats': {\n",
        "                    'sentence_count': len(processed_sentences),\n",
        "                    'word_count': doc_word_count,\n",
        "                    'character_count': len(text)\n",
        "                }\n",
        "            }\n",
        "\n",
        "            return document_data\n",
        "\n",
        "        return None\n",
        "\n",
        "    def compute_final_statistics(self) -> Dict[str, float]:\n",
        "        \"\"\"Compute all required corpus statistics\"\"\"\n",
        "        if self.total_sentences == 0:\n",
        "            return {}\n",
        "\n",
        "        # Calculate averages\n",
        "        avg_sentence_length = (sum(self.sentence_lengths) / len(self.sentence_lengths)) if self.sentence_lengths else 0\n",
        "        avg_word_length = (sum(self.word_lengths) / len(self.word_lengths)) if self.word_lengths else 0\n",
        "\n",
        "        # Calculate Type-Token Ratio\n",
        "        unique_tokens = len(self.vocabulary)\n",
        "        total_tokens = self.total_words\n",
        "        ttr = unique_tokens / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'total_sentences': self.total_sentences,\n",
        "            'total_words': self.total_words,\n",
        "            'total_characters': self.total_characters,\n",
        "            'average_sentence_length': round(avg_sentence_length, 2),\n",
        "            'average_word_length': round(avg_word_length, 2),\n",
        "            'type_token_ratio': round(ttr, 4),\n",
        "            'vocabulary_size': unique_tokens,\n",
        "            'processed_documents': self.processed_documents\n",
        "        }\n",
        "\n",
        "    def save_checkpoint(self, output_dir: str):\n",
        "        \"\"\"Save current progress as checkpoint\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        checkpoint_file = os.path.join(output_dir, f\"checkpoint_{self.processed_documents}.pkl\")\n",
        "\n",
        "        checkpoint_data = {\n",
        "            'tokenized_data': self.tokenized_data,\n",
        "            'processed_documents': self.processed_documents,\n",
        "            'timestamp': time.time()\n",
        "        }\n",
        "\n",
        "        with open(checkpoint_file, 'wb') as f:\n",
        "            pickle.dump(checkpoint_data, f)\n",
        "\n",
        "    def save_data_and_statistics(self, output_dir: str = \"hindi_corpus_output\"):\n",
        "        \"\"\"Save tokenized data and statistics to files - optimized for Colab\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save current tokenized data if any\n",
        "        if self.tokenized_data:\n",
        "            json_file = os.path.join(output_dir, \"tokenized_data_final.json\")\n",
        "            with open(json_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.tokenized_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Compute and save statistics\n",
        "        stats = self.compute_final_statistics()\n",
        "\n",
        "        # Save statistics as JSON\n",
        "        stats_file = os.path.join(output_dir, \"corpus_statistics.json\")\n",
        "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save top vocabulary (memory-friendly)\n",
        "        vocab_file = os.path.join(output_dir, \"vocabulary_top1000.json\")\n",
        "        vocab_dict = dict(self.vocabulary.most_common(1000))  # Top 1K words for Colab\n",
        "        with open(vocab_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(vocab_dict, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save detailed report\n",
        "        report_file = os.path.join(output_dir, \"statistics_report.txt\")\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"NLP ASSIGNMENT 1 - HINDI CORPUS STATISTICS REPORT\\n\")\n",
        "            f.write(\"=\" * 55 + \"\\n\\n\")\n",
        "            f.write(\"Dataset: ai4bharat/IndicCorpV2 (Hindi)\\n\")\n",
        "            f.write(\"File: hi-1.txt (streaming)\\n\")\n",
        "            f.write(f\"Processing Scale: {stats['processed_documents']:,} documents\\n\")\n",
        "            f.write(f\"Memory Management: Colab-optimized\\n\\n\")\n",
        "\n",
        "            f.write(\"ASSIGNMENT REQUIREMENTS (Task 1d):\\n\")\n",
        "            f.write(\"-\" * 35 + \"\\n\")\n",
        "            f.write(f\"i.   Total number of sentences: {stats['total_sentences']:,}\\n\")\n",
        "            f.write(f\"ii.  Total number of words: {stats['total_words']:,}\\n\")\n",
        "            f.write(f\"iii. Total number of characters: {stats['total_characters']:,}\\n\")\n",
        "            f.write(f\"iv.  Average Sentence Length: {stats['average_sentence_length']} words per sentence\\n\")\n",
        "            f.write(f\"v.   Average word length: {stats['average_word_length']} characters per word\\n\")\n",
        "            f.write(f\"vi.  Type/Token Ratio (TTR): {stats['type_token_ratio']}\\n\\n\")\n",
        "\n",
        "            f.write(\"EXTENDED STATISTICS:\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "            f.write(f\"Vocabulary size (unique tokens): {stats['vocabulary_size']:,}\\n\")\n",
        "            f.write(f\"Processed documents: {stats['processed_documents']:,}\\n\")\n",
        "            f.write(f\"Longest sentence: {max(self.sentence_lengths) if self.sentence_lengths else 0} words\\n\")\n",
        "            f.write(f\"Shortest sentence: {min(self.sentence_lengths) if self.sentence_lengths else 0} words\\n\")\n",
        "            f.write(f\"Longest word: {max(self.word_lengths) if self.word_lengths else 0} characters\\n\\n\")\n",
        "\n",
        "            f.write(\"TOP 20 MOST FREQUENT TOKENS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            for i, (word, freq) in enumerate(self.vocabulary.most_common(20), 1):\n",
        "                f.write(f\"{i:2d}. {word}: {freq:,}\\n\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "# -------------------- Main Processing Function --------------------\n",
        "def main():\n",
        "    print(\"NLP ASSIGNMENT 1: Text Preprocessing with Hindi IndicCorpV2\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Dataset: ai4bharat/IndicCorpV2 (Hindi - hi-1.txt)\")\n",
        "    print(\"Environment: Google Colab Optimized\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Stream Hindi dataset from Hugging Face without full download\n",
        "        print(\"Loading dataset stream...\")\n",
        "        hindi_dataset = load_dataset(\n",
        "            \"text\",\n",
        "            data_files=\"https://huggingface.co/datasets/ai4bharat/IndicCorpV2/resolve/main/data/hi-1.txt\",\n",
        "            split=\"train\",\n",
        "            streaming=True\n",
        "        )\n",
        "        print(\"‚úÖ Dataset stream loaded successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading dataset: {e}\")\n",
        "        print(\"Trying alternative approach...\")\n",
        "        # Alternative: use a smaller sample or local file\n",
        "        return\n",
        "\n",
        "    # Initialize corpus statistics\n",
        "    corpus_stats = CorpusStatistics()\n",
        "\n",
        "    print(f\"\\nInitial memory usage: {get_memory_usage():.1f}MB\")\n",
        "    print(\"\\nProcessing Hindi dataset...\\n\")\n",
        "\n",
        "    # Process examples with memory management\n",
        "    print(\"SAMPLE PROCESSING (First 3 examples):\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    sample_count = 0\n",
        "    processed_count = 0\n",
        "    max_documents = 10000  # Limit for Colab (adjust as needed)\n",
        "\n",
        "    try:\n",
        "        for i, example in enumerate(hindi_dataset):\n",
        "            # Skip if 'text' is missing or just whitespace\n",
        "            if 'text' not in example or not example['text'].strip():\n",
        "                continue\n",
        "\n",
        "            text = example['text'].strip()\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            # Show sample processing for first 3 examples\n",
        "            if sample_count < 3:\n",
        "                print(f\"\\n--- Example {sample_count + 1} ---\")\n",
        "                print(\"Original Paragraph:\")\n",
        "                print(text[:150] + \"...\" if len(text) > 150 else text)\n",
        "\n",
        "                sentences = sentence_split(text)\n",
        "                print(f\"\\nSentences found: {len(sentences)}\")\n",
        "\n",
        "                if sentences:\n",
        "                    print(\"\\nFirst sentence tokens:\")\n",
        "                    tokens = word_tokenize(sentences[0])\n",
        "                    print(tokens[:10])  # Show first 10 tokens\n",
        "                    if len(tokens) > 10:\n",
        "                        print(f\"... and {len(tokens) - 10} more tokens\")\n",
        "\n",
        "                sample_count += 1\n",
        "\n",
        "            # Process document for statistics\n",
        "            processed_doc = corpus_stats.process_document(text, processed_count)\n",
        "            if processed_doc:\n",
        "                corpus_stats.tokenized_data.append(processed_doc)\n",
        "                processed_count += 1\n",
        "\n",
        "            # Memory management and progress updates\n",
        "            if processed_count > 0 and processed_count % 1000 == 0:\n",
        "                print(f\"\\nüìä Progress: {processed_count:,} documents processed\")\n",
        "                print(f\"   Sentences: {corpus_stats.total_sentences:,}\")\n",
        "                print(f\"   Words: {corpus_stats.total_words:,}\")\n",
        "                print(f\"   Memory: {get_memory_usage():.1f}MB\")\n",
        "\n",
        "                # Check memory and save checkpoint if needed\n",
        "                corpus_stats.check_memory_and_save()\n",
        "\n",
        "            # Stop if we reach the limit\n",
        "            if processed_count >= max_documents:\n",
        "                print(f\"\\nüõë Reached processing limit of {max_documents:,} documents\")\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Processing interrupted: {e}\")\n",
        "        print(\"Continuing with data processed so far...\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Processing complete!\")\n",
        "    print(f\"üìä Processed {processed_count:,} documents\")\n",
        "    print(f\"üíæ Current memory usage: {get_memory_usage():.1f}MB\")\n",
        "\n",
        "    # Save data and compute statistics\n",
        "    print(\"\\nüíæ Saving tokenized data and computing statistics...\")\n",
        "    try:\n",
        "        stats = corpus_stats.save_data_and_statistics()\n",
        "\n",
        "        # Display final statistics\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"FINAL CORPUS STATISTICS (Assignment Task 1d)\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"i.   Total number of sentences: {stats['total_sentences']:,}\")\n",
        "        print(f\"ii.  Total number of words: {stats['total_words']:,}\")\n",
        "        print(f\"iii. Total number of characters: {stats['total_characters']:,}\")\n",
        "        print(f\"iv.  Average Sentence Length: {stats['average_sentence_length']} words per sentence\")\n",
        "        print(f\"v.   Average word length: {stats['average_word_length']} characters per word\")\n",
        "        print(f\"vi.  Type/Token Ratio (TTR): {stats['type_token_ratio']}\")\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"Vocabulary size: {stats['vocabulary_size']:,} unique tokens\")\n",
        "        print(f\"Processed documents: {stats['processed_documents']:,}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(f\"\\nüéØ ASSIGNMENT TASK 1 COMPLETED!\")\n",
        "        print(f\"üìÅ Check 'hindi_corpus_output' directory for:\")\n",
        "        print(f\"   üìÑ tokenized_data_final.json - Tokenized documents\")\n",
        "        print(f\"   üìä corpus_statistics.json - All computed statistics\")\n",
        "        print(f\"   üìù statistics_report.txt - Detailed human-readable report\")\n",
        "        print(f\"   üî§ vocabulary_top1000.json - Most frequent words\")\n",
        "\n",
        "        print(f\"\\n‚úÖ All requirements completed:\")\n",
        "        print(f\"   a. ‚úÖ Downloaded and processed {stats['processed_documents']:,} Hindi documents\")\n",
        "        print(f\"   b. ‚úÖ Tokenized {stats['total_sentences']:,} sentences into {stats['total_words']:,} words\")\n",
        "        print(f\"   c. ‚úÖ Saved all tokenized data\")\n",
        "        print(f\"   d. ‚úÖ Computed comprehensive corpus statistics\")\n",
        "\n",
        "        print(f\"\\nüìà CORPUS INSIGHTS:\")\n",
        "        print(f\"   üî§ Vocabulary richness: {stats['vocabulary_size']:,} unique tokens\")\n",
        "        print(f\"   üìù Text density: {stats['total_characters']:,} characters processed\")\n",
        "        print(f\"   üîÑ Language diversity: TTR = {stats['type_token_ratio']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving results: {e}\")\n",
        "        return\n",
        "\n",
        "# -------------------- Installation and Setup --------------------\n",
        "def install_requirements():\n",
        "    \"\"\"Install required packages for Colab\"\"\"\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    packages = ['datasets', 'psutil']\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"‚úÖ {package} already installed\")\n",
        "        except ImportError:\n",
        "            print(f\"üì¶ Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# -------------------- Run the script --------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment the line below if you need to install packages\n",
        "    # install_requirements()\n",
        "\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CleWOxLNgYol"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}