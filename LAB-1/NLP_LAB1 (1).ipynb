{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "add3ecc0001947009bc14666bfbca8fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a912ca5563de4ca188b69a839e1c2ab2",
              "IPY_MODEL_906465f81ad34ad48d043628590272a8",
              "IPY_MODEL_16ca65eca2d34a1d81f59be7eeef37db"
            ],
            "layout": "IPY_MODEL_a9769ab34a0c405992d037c750374bbd"
          }
        },
        "a912ca5563de4ca188b69a839e1c2ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1e32f740d404fdeb4c8dad3f0b1be72",
            "placeholder": "​",
            "style": "IPY_MODEL_ef640178cc264a6cbd58a3d71eca0320",
            "value": "⏳ Saving: "
          }
        },
        "906465f81ad34ad48d043628590272a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94f1bdb5df944e62ae58b1508c422314",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_604b4e6710274ba580f1aab19afc6837",
            "value": 1
          }
        },
        "16ca65eca2d34a1d81f59be7eeef37db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a98a1df1c184b8f8e563e7bca97738d",
            "placeholder": "​",
            "style": "IPY_MODEL_307ead61e6244cc7a556fcba97cd74b2",
            "value": " 494785/? [00:57&lt;00:00, 7360.92 lines/s]"
          }
        },
        "a9769ab34a0c405992d037c750374bbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1e32f740d404fdeb4c8dad3f0b1be72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef640178cc264a6cbd58a3d71eca0320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94f1bdb5df944e62ae58b1508c422314": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "604b4e6710274ba580f1aab19afc6837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a98a1df1c184b8f8e563e7bca97738d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "307ead61e6244cc7a556fcba97cd74b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Stream Hindi dataset from Hugging Face without full download\n",
        "hindi_dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files=\"https://huggingface.co/datasets/ai4bharat/IndicCorpV2/resolve/main/data/hi-1.txt\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Example: read first 5 lines\n",
        "for i, item in enumerate(hindi_dataset):\n",
        "    print(item[\"text\"])\n",
        "    if i == 4:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb4FAPZ2duGs",
        "outputId": "80749f82-a5e6-47c7-cb13-b5726a0c4f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था। हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी। 1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी। उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था। उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ. देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है। ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "\n",
            "जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "# -------------------- Sentence Tokenizer --------------------\n",
        "def sentence_split(paragraph):\n",
        "    sentence_endings = ['.', '?', '!', '।']\n",
        "    sentences = []\n",
        "    current = ''\n",
        "    for char in paragraph:\n",
        "        current += char\n",
        "        if char in sentence_endings:\n",
        "            if current.strip():\n",
        "                sentences.append(current.strip())\n",
        "                current = ''\n",
        "    if current.strip():\n",
        "        sentences.append(current.strip())\n",
        "    return sentences\n",
        "\n",
        "# -------------------- Word Tokenizer --------------------\n",
        "def is_devanagari(char):\n",
        "    return '\\u0900' <= char <= '\\u097F'\n",
        "\n",
        "def is_digit_or_latin(char):\n",
        "    return char.isdigit() or ('\\u0041' <= char <= '\\u007A') or ('\\u0041' <= char <= '\\u005A')\n",
        "\n",
        "def word_tokenize(sentence):\n",
        "    tokens = []\n",
        "    word = ''\n",
        "\n",
        "    for char in sentence:\n",
        "        if is_devanagari(char) or is_digit_or_latin(char) or char in ['@', '.', '/', ':', '-', '_']:\n",
        "            word += char\n",
        "        else:\n",
        "            if word:\n",
        "                tokens.append(word)\n",
        "                word = ''\n",
        "            if char.strip():\n",
        "                tokens.append(char)\n",
        "    if word:\n",
        "        tokens.append(word)\n",
        "\n",
        "    # Group final tokens for dates, decimals, emails, URLs\n",
        "    final_tokens = []\n",
        "    for token in tokens:\n",
        "        if '/' in token and all(part.isdigit() for part in token.split('/') if part):  # date\n",
        "            final_tokens.append(token)\n",
        "        elif '.' in token and token.replace('.', '', 1).isdigit():  # decimal\n",
        "            final_tokens.append(token)\n",
        "        elif '@' in token and '.' in token:  # email\n",
        "            final_tokens.append(token)\n",
        "        elif token.startswith('http') or 'www.' in token:  # url\n",
        "            final_tokens.append(token)\n",
        "        else:\n",
        "            final_tokens.append(token)\n",
        "    return final_tokens\n",
        "\n",
        "# -------------------- Example Dataset --------------------\n",
        "# Replace this with your actual dataset loader\n",
        "\n",
        "\n",
        "# -------------------- Process and Print Tokenization --------------------\n",
        "print(\"Processing Hindi dataset...\\n\")\n",
        "for i, example in enumerate(hindi_dataset):\n",
        "    if 'text' not in example:\n",
        "        continue\n",
        "\n",
        "    text = example['text']\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(\"Original Paragraph:\\n\", text)\n",
        "\n",
        "    sentences = sentence_split(text)\n",
        "\n",
        "    print(\"\\nTokenized Sentences:\")\n",
        "    for sent in sentences:\n",
        "        print(\"-\", sent)\n",
        "\n",
        "    print(\"\\nTokenized Words (per sentence):\")\n",
        "    for sent in sentences:\n",
        "        print(word_tokenize(sent))\n",
        "\n",
        "    if i == 2:  # Show only first 3 examples\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOr5aldTeF4W",
        "outputId": "88cc6fe1-8f62-4b18-d999-05474e108089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Hindi dataset...\n",
            "\n",
            "\n",
            "--- Example 1 ---\n",
            "Original Paragraph:\n",
            " लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "Tokenized Sentences:\n",
            "- लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "Tokenized Words (per sentence):\n",
            "['लोगों', 'को', 'बिलों', 'संबंधी', 'सुविधा', 'देना', 'ही', 'उनका', 'काम']\n",
            "\n",
            "--- Example 2 ---\n",
            "Original Paragraph:\n",
            " \n",
            "\n",
            "Tokenized Sentences:\n",
            "\n",
            "Tokenized Words (per sentence):\n",
            "\n",
            "--- Example 3 ---\n",
            "Original Paragraph:\n",
            " इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था। हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी। 1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी। उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था। उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ. देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है। ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "\n",
            "Tokenized Sentences:\n",
            "- इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था।\n",
            "- हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी।\n",
            "- 1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी।\n",
            "- उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था।\n",
            "- उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ.\n",
            "- देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है।\n",
            "- ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "\n",
            "Tokenized Words (per sentence):\n",
            "['इनेलो', '1987', 'में', 'उस', 'वक्त', 'ऐसे', 'ही', 'दोराहे', 'पर', 'खड़ी', 'थी', ',', 'जब', 'पूर्व', 'उपप्रधानमंत्री', 'देवीलाल', 'ने', 'अपने', 'पुत्र', 'ओमप्रकाश', 'चौटाला', 'को', 'अपना', 'राजनीतिक', 'उत्तराधिकारी', 'घोषित', 'किया', 'था।']\n",
            "['हालांकि', 'तब', 'पार्टी', 'पर', 'देवीलाल', 'की', 'मजबूत', 'पकड़', 'के', 'चलते', 'पार्टी', 'टूटने', 'से', 'बच', 'गई', 'थी।']\n",
            "['1989', 'में', 'देवीलाल', 'केन्द्र', 'की', 'राजनीति', 'में', 'सक्रिय', 'हो', 'गए', 'थे', 'और', 'उनके', 'उपप्रधानमंत्री', 'बनने', 'के', 'पश्चात्', 'उनके', 'तीन', 'बेटों', 'जगदीश', 'सिंह', ',', 'रणजीत', 'सिंह', 'और', 'ओमप्रकाश', 'चौटाला', 'में', 'से', 'रणजीत', 'और', 'ओमप्रकाश', 'के', 'बीच', 'हरियाणा', 'में', 'उनकी', 'राजनीतिक', 'विरासत', 'को', 'लेकर', 'जंग', 'शुरू', 'हो', 'गई', 'थी।']\n",
            "['उन', 'परिस्थितियों', 'में', 'देवीलाल', 'ने', 'कड़ा', 'निर्णय', 'लेते', 'हुए', 'पार्टी', 'की', 'बागडोर', 'ओमप्रकाश', 'चौटाला', 'के', 'हवाले', 'कर', 'दी', 'थी', ',', 'जिसके', 'बाद', 'रणजीत', 'की', 'बगावत', 'का', 'असर', 'पार्टी', ',', 'संगठन', 'और', 'उनकी', 'सरकार', 'पर', 'भी', 'पड़ा', 'था।']\n",
            "['उस', 'समय', 'रणजीत', 'की', 'नाराजगी', 'के', 'चलते', 'उनके', 'समर्थन', 'में', 'कई', 'कैबिनेट', 'मंत्रियों', 'ने', 'इस्तीफे', 'दे', 'दिए', 'थे', 'किन्तु', 'तब', 'पार्टी', 'सुप्रीमो', 'चौ.']\n",
            "['देवीलाल', 'की', 'हरियाणा', 'की', 'जनता', 'पर', 'इतनी', 'मजबूत', 'पकड़', 'थी', 'कि', 'ओमप्रकाश', 'चौटाला', 'को', 'उत्तराधिकारी', 'बनाने', 'के', 'उनके', 'फैसले', 'का', 'जनता', 'के', 'बीच', 'कोई', 'खास', 'विरोध', 'नहीं', 'हुआ', 'था', 'लेकिन', 'आज', 'स्थिति', 'बिल्कुल', 'विपरीत', 'है।']\n",
            "['ओमप्रकाश', 'चौटाला', 'पिछले', 'काफी', 'समय', 'से', 'जेल', 'में', 'हैं', 'और', 'जेल', 'में', 'रहते', 'पार्टी', 'के', 'साथ-साथ', 'परिवार', 'पर', 'भी', 'उनकी', 'पकड़', 'काफी', 'ढ़ीली', 'हो', 'गई', 'है', ',', 'इसी', 'कारण', 'उनमें', 'अब', 'देवीलाल', 'जैसा', 'वो', 'सामर्थ्य', 'नजर', 'नहीं', 'आता', 'कि', 'वे', 'अपने', 'फैसलों', 'को', 'बगैर', 'किसी', 'प्रतिरोध', 'के', 'लागू', 'करा', 'सकें।']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# -------------------- Sentence Tokenizer --------------------\n",
        "def sentence_split(paragraph):\n",
        "    sentence_endings = ['.', '?', '!', '।']\n",
        "    sentences = []\n",
        "    current = ''\n",
        "    for char in paragraph:\n",
        "        current += char\n",
        "        if char in sentence_endings:\n",
        "            if current.strip():\n",
        "                sentences.append(current.strip())\n",
        "                current = ''\n",
        "    if current.strip():\n",
        "        sentences.append(current.strip())\n",
        "    return sentences\n",
        "\n",
        "# -------------------- Word Tokenizer using regex --------------------\n",
        "def word_tokenize(sentence):\n",
        "    # Regex explanation:\n",
        "    # - Devanagari block: \\u0900-\\u097F\n",
        "    # - Latin letters and digits: a-zA-Z0-9\n",
        "    # - URLs, emails, decimals, dates, special characters handled via patterns\n",
        "    pattern = r'''\n",
        "        (https?://[^\\s]+) |                     # URLs\n",
        "        (www\\.[^\\s]+) |                         # www. URLs\n",
        "        (\\w+@\\w+\\.\\w+) |                        # Emails\n",
        "        (\\d{1,2}/\\d{1,2}/\\d{2,4}) |             # Dates (e.g., 12/01/2023)\n",
        "        (\\d+\\.\\d+) |                            # Decimal numbers\n",
        "        ([\\u0900-\\u097F]+) |                    # Devanagari words\n",
        "        ([a-zA-Z0-9_-]+) |                      # Latin words or numbers\n",
        "        ([^\\s])                                 # Other single non-whitespace characters (punctuation etc.)\n",
        "    '''\n",
        "    tokens = re.findall(pattern, sentence, re.VERBOSE)\n",
        "\n",
        "    # Flatten the list of matched groups (each match is a tuple of possible groups)\n",
        "    flat_tokens = [token for group in tokens for token in group if token]\n",
        "    return flat_tokens\n",
        "\n",
        "# -------------------- Example Usage --------------------\n",
        "# Replace `hindi_dataset` with your streaming dataset\n",
        "\n",
        "print(\"Processing Hindi dataset...\\n\")\n",
        "for i, example in enumerate(hindi_dataset):\n",
        "    if 'text' not in example:\n",
        "        continue\n",
        "\n",
        "    text = example['text']\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(\"Original Paragraph:\\n\", text)\n",
        "\n",
        "    sentences = sentence_split(text)\n",
        "\n",
        "    print(\"\\nTokenized Sentences:\")\n",
        "    for sent in sentences:\n",
        "        print(\"-\", sent)\n",
        "\n",
        "    print(\"\\nTokenized Words (per sentence):\")\n",
        "    for sent in sentences:\n",
        "        print(word_tokenize(sent))\n",
        "\n",
        "    if i == 4:  # Show only first 3 examples\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYQA9n9kf8Ll",
        "outputId": "49b61d57-5d9d-4385-ab21-af16c5aa6613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Hindi dataset...\n",
            "\n",
            "\n",
            "--- Example 1 ---\n",
            "Original Paragraph:\n",
            " लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "Tokenized Sentences:\n",
            "- लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "Tokenized Words (per sentence):\n",
            "['लोगों', 'को', 'बिलों', 'संबंधी', 'सुविधा', 'देना', 'ही', 'उनका', 'काम']\n",
            "\n",
            "--- Example 2 ---\n",
            "Original Paragraph:\n",
            " \n",
            "\n",
            "Tokenized Sentences:\n",
            "\n",
            "Tokenized Words (per sentence):\n",
            "\n",
            "--- Example 3 ---\n",
            "Original Paragraph:\n",
            " इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था। हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी। 1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी। उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था। उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ. देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है। ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "\n",
            "Tokenized Sentences:\n",
            "- इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था।\n",
            "- हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी।\n",
            "- 1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी।\n",
            "- उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था।\n",
            "- उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ.\n",
            "- देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है।\n",
            "- ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "\n",
            "Tokenized Words (per sentence):\n",
            "['इनेलो', '1987', 'में', 'उस', 'वक्त', 'ऐसे', 'ही', 'दोराहे', 'पर', 'खड़ी', 'थी', ',', 'जब', 'पूर्व', 'उपप्रधानमंत्री', 'देवीलाल', 'ने', 'अपने', 'पुत्र', 'ओमप्रकाश', 'चौटाला', 'को', 'अपना', 'राजनीतिक', 'उत्तराधिकारी', 'घोषित', 'किया', 'था।']\n",
            "['हालांकि', 'तब', 'पार्टी', 'पर', 'देवीलाल', 'की', 'मजबूत', 'पकड़', 'के', 'चलते', 'पार्टी', 'टूटने', 'से', 'बच', 'गई', 'थी।']\n",
            "['1989', 'में', 'देवीलाल', 'केन्द्र', 'की', 'राजनीति', 'में', 'सक्रिय', 'हो', 'गए', 'थे', 'और', 'उनके', 'उपप्रधानमंत्री', 'बनने', 'के', 'पश्चात्', 'उनके', 'तीन', 'बेटों', 'जगदीश', 'सिंह', ',', 'रणजीत', 'सिंह', 'और', 'ओमप्रकाश', 'चौटाला', 'में', 'से', 'रणजीत', 'और', 'ओमप्रकाश', 'के', 'बीच', 'हरियाणा', 'में', 'उनकी', 'राजनीतिक', 'विरासत', 'को', 'लेकर', 'जंग', 'शुरू', 'हो', 'गई', 'थी।']\n",
            "['उन', 'परिस्थितियों', 'में', 'देवीलाल', 'ने', 'कड़ा', 'निर्णय', 'लेते', 'हुए', 'पार्टी', 'की', 'बागडोर', 'ओमप्रकाश', 'चौटाला', 'के', 'हवाले', 'कर', 'दी', 'थी', ',', 'जिसके', 'बाद', 'रणजीत', 'की', 'बगावत', 'का', 'असर', 'पार्टी', ',', 'संगठन', 'और', 'उनकी', 'सरकार', 'पर', 'भी', 'पड़ा', 'था।']\n",
            "['उस', 'समय', 'रणजीत', 'की', 'नाराजगी', 'के', 'चलते', 'उनके', 'समर्थन', 'में', 'कई', 'कैबिनेट', 'मंत्रियों', 'ने', 'इस्तीफे', 'दे', 'दिए', 'थे', 'किन्तु', 'तब', 'पार्टी', 'सुप्रीमो', 'चौ', '.']\n",
            "['देवीलाल', 'की', 'हरियाणा', 'की', 'जनता', 'पर', 'इतनी', 'मजबूत', 'पकड़', 'थी', 'कि', 'ओमप्रकाश', 'चौटाला', 'को', 'उत्तराधिकारी', 'बनाने', 'के', 'उनके', 'फैसले', 'का', 'जनता', 'के', 'बीच', 'कोई', 'खास', 'विरोध', 'नहीं', 'हुआ', 'था', 'लेकिन', 'आज', 'स्थिति', 'बिल्कुल', 'विपरीत', 'है।']\n",
            "['ओमप्रकाश', 'चौटाला', 'पिछले', 'काफी', 'समय', 'से', 'जेल', 'में', 'हैं', 'और', 'जेल', 'में', 'रहते', 'पार्टी', 'के', 'साथ', '-', 'साथ', 'परिवार', 'पर', 'भी', 'उनकी', 'पकड़', 'काफी', 'ढ़ीली', 'हो', 'गई', 'है', ',', 'इसी', 'कारण', 'उनमें', 'अब', 'देवीलाल', 'जैसा', 'वो', 'सामर्थ्य', 'नजर', 'नहीं', 'आता', 'कि', 'वे', 'अपने', 'फैसलों', 'को', 'बगैर', 'किसी', 'प्रतिरोध', 'के', 'लागू', 'करा', 'सकें।']\n",
            "\n",
            "--- Example 4 ---\n",
            "Original Paragraph:\n",
            " \n",
            "\n",
            "Tokenized Sentences:\n",
            "\n",
            "Tokenized Words (per sentence):\n",
            "\n",
            "--- Example 5 ---\n",
            "Original Paragraph:\n",
            " जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n",
            "\n",
            "Tokenized Sentences:\n",
            "- जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n",
            "\n",
            "Tokenized Words (per sentence):\n",
            "['जहां', 'आई', 'थी', 'तबाही', 'उस', 'घाटी', 'क्षेत्र', 'में', 'खतरा', 'ज्यादा']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# -------------------- Sentence Tokenizer --------------------\n",
        "def sentence_split(paragraph):\n",
        "    sentence_endings = ['.', '?', '!', '।']\n",
        "    sentences = []\n",
        "    current = ''\n",
        "    for char in paragraph:\n",
        "        current += char\n",
        "        if char in sentence_endings:\n",
        "            if current.strip():\n",
        "                sentences.append(current.strip())\n",
        "                current = ''\n",
        "    if current.strip():\n",
        "        sentences.append(current.strip())\n",
        "    return sentences\n",
        "\n",
        "# -------------------- Word Tokenizer using regex --------------------\n",
        "def word_tokenize(sentence):\n",
        "    # Regex explanation:\n",
        "    # - Group1: URLs starting with http(s)\n",
        "    # - Group2: URLs starting with www\n",
        "    # - Group3: Emails\n",
        "    # - Group4: Dates like 12/01/2024\n",
        "    # - Group5: Decimal numbers\n",
        "    # - Group6: Devanagari words\n",
        "    # - Group7: Latin/English words or digits\n",
        "    # - Group8: Any other non-whitespace single character (e.g., punctuation)\n",
        "    pattern = r'''\n",
        "        (https?://[^\\s]+) |                     # Group 1: URLs\n",
        "        (www\\.[^\\s]+) |                         # Group 2: www URLs\n",
        "        (\\w+@\\w+\\.\\w+) |                        # Group 3: Emails\n",
        "        (\\d{1,2}/\\d{1,2}/\\d{2,4}) |             # Group 4: Dates\n",
        "        (\\d+\\.\\d+) |                            # Group 5: Decimal numbers\n",
        "        ([\\u0900-\\u097F]+) |                    # Group 6: Hindi (Devanagari) words\n",
        "        ([a-zA-Z0-9_-]+) |                      # Group 7: Latin words/digits\n",
        "        ([^\\s])                                 # Group 8: Other single chars\n",
        "    '''\n",
        "    tokens = re.findall(pattern, sentence, re.VERBOSE)\n",
        "    flat_tokens = [token for group in tokens for token in group if token]\n",
        "    return flat_tokens\n",
        "\n",
        "# -------------------- Hindi Dataset Example Processing --------------------\n",
        "print(\"Processing Hindi dataset...\\n\")\n",
        "for i, example in enumerate(hindi_dataset):\n",
        "    # Skip if 'text' is missing or just whitespace\n",
        "    if 'text' not in example or not example['text'].strip():\n",
        "        continue\n",
        "\n",
        "    text = example['text'].strip()\n",
        "    if not text:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n--- Example  ---\")\n",
        "    print(\"Original Paragraph:\\n\", text)\n",
        "\n",
        "    sentences = sentence_split(text)\n",
        "\n",
        "    print(\"\\nTokenized Sentences:\")\n",
        "    for sent in sentences:\n",
        "        print(\"-\", sent)\n",
        "\n",
        "    print(\"\\nTokenized Words (per sentence):\")\n",
        "    for sent in sentences:\n",
        "        print(word_tokenize(sent))\n",
        "\n",
        "    if i == 4:  # Limit to 5 examples\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8V5w6u0n4Ym",
        "outputId": "615ca6cd-eb83-4cb2-f07f-313fc35a2e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Hindi dataset...\n",
            "\n",
            "\n",
            "--- Example  ---\n",
            "Original Paragraph:\n",
            " लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "Tokenized Sentences:\n",
            "- लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "Tokenized Words (per sentence):\n",
            "['लोगों', 'को', 'बिलों', 'संबंधी', 'सुविधा', 'देना', 'ही', 'उनका', 'काम']\n",
            "\n",
            "--- Example  ---\n",
            "Original Paragraph:\n",
            " इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था। हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी। 1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी। उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था। उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ. देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है। ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "\n",
            "Tokenized Sentences:\n",
            "- इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था।\n",
            "- हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी।\n",
            "- 1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी।\n",
            "- उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था।\n",
            "- उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ.\n",
            "- देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है।\n",
            "- ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "\n",
            "Tokenized Words (per sentence):\n",
            "['इनेलो', '1987', 'में', 'उस', 'वक्त', 'ऐसे', 'ही', 'दोराहे', 'पर', 'खड़ी', 'थी', ',', 'जब', 'पूर्व', 'उपप्रधानमंत्री', 'देवीलाल', 'ने', 'अपने', 'पुत्र', 'ओमप्रकाश', 'चौटाला', 'को', 'अपना', 'राजनीतिक', 'उत्तराधिकारी', 'घोषित', 'किया', 'था।']\n",
            "['हालांकि', 'तब', 'पार्टी', 'पर', 'देवीलाल', 'की', 'मजबूत', 'पकड़', 'के', 'चलते', 'पार्टी', 'टूटने', 'से', 'बच', 'गई', 'थी।']\n",
            "['1989', 'में', 'देवीलाल', 'केन्द्र', 'की', 'राजनीति', 'में', 'सक्रिय', 'हो', 'गए', 'थे', 'और', 'उनके', 'उपप्रधानमंत्री', 'बनने', 'के', 'पश्चात्', 'उनके', 'तीन', 'बेटों', 'जगदीश', 'सिंह', ',', 'रणजीत', 'सिंह', 'और', 'ओमप्रकाश', 'चौटाला', 'में', 'से', 'रणजीत', 'और', 'ओमप्रकाश', 'के', 'बीच', 'हरियाणा', 'में', 'उनकी', 'राजनीतिक', 'विरासत', 'को', 'लेकर', 'जंग', 'शुरू', 'हो', 'गई', 'थी।']\n",
            "['उन', 'परिस्थितियों', 'में', 'देवीलाल', 'ने', 'कड़ा', 'निर्णय', 'लेते', 'हुए', 'पार्टी', 'की', 'बागडोर', 'ओमप्रकाश', 'चौटाला', 'के', 'हवाले', 'कर', 'दी', 'थी', ',', 'जिसके', 'बाद', 'रणजीत', 'की', 'बगावत', 'का', 'असर', 'पार्टी', ',', 'संगठन', 'और', 'उनकी', 'सरकार', 'पर', 'भी', 'पड़ा', 'था।']\n",
            "['उस', 'समय', 'रणजीत', 'की', 'नाराजगी', 'के', 'चलते', 'उनके', 'समर्थन', 'में', 'कई', 'कैबिनेट', 'मंत्रियों', 'ने', 'इस्तीफे', 'दे', 'दिए', 'थे', 'किन्तु', 'तब', 'पार्टी', 'सुप्रीमो', 'चौ', '.']\n",
            "['देवीलाल', 'की', 'हरियाणा', 'की', 'जनता', 'पर', 'इतनी', 'मजबूत', 'पकड़', 'थी', 'कि', 'ओमप्रकाश', 'चौटाला', 'को', 'उत्तराधिकारी', 'बनाने', 'के', 'उनके', 'फैसले', 'का', 'जनता', 'के', 'बीच', 'कोई', 'खास', 'विरोध', 'नहीं', 'हुआ', 'था', 'लेकिन', 'आज', 'स्थिति', 'बिल्कुल', 'विपरीत', 'है।']\n",
            "['ओमप्रकाश', 'चौटाला', 'पिछले', 'काफी', 'समय', 'से', 'जेल', 'में', 'हैं', 'और', 'जेल', 'में', 'रहते', 'पार्टी', 'के', 'साथ', '-', 'साथ', 'परिवार', 'पर', 'भी', 'उनकी', 'पकड़', 'काफी', 'ढ़ीली', 'हो', 'गई', 'है', ',', 'इसी', 'कारण', 'उनमें', 'अब', 'देवीलाल', 'जैसा', 'वो', 'सामर्थ्य', 'नजर', 'नहीं', 'आता', 'कि', 'वे', 'अपने', 'फैसलों', 'को', 'बगैर', 'किसी', 'प्रतिरोध', 'के', 'लागू', 'करा', 'सकें।']\n",
            "\n",
            "--- Example  ---\n",
            "Original Paragraph:\n",
            " जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n",
            "\n",
            "Tokenized Sentences:\n",
            "- जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n",
            "\n",
            "Tokenized Words (per sentence):\n",
            "['जहां', 'आई', 'थी', 'तबाही', 'उस', 'घाटी', 'क्षेत्र', 'में', 'खतरा', 'ज्यादा']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j58RL8sWpXfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "\n",
        "# -------------------- Sentence Tokenizer --------------------\n",
        "def sentence_split(paragraph):\n",
        "    sentence_endings = ['.', '?', '!', '।']\n",
        "    sentences = []\n",
        "    current = ''\n",
        "    for char in paragraph:\n",
        "        current += char\n",
        "        if char in sentence_endings:\n",
        "            if current.strip():\n",
        "                sentences.append(current.strip())\n",
        "                current = ''\n",
        "    if current.strip():\n",
        "        sentences.append(current.strip())\n",
        "    return sentences\n",
        "\n",
        "# -------------------- Word Tokenizer using regex --------------------\n",
        "def word_tokenize(sentence):\n",
        "    pattern = r'''\n",
        "        (https?://[^\\s]+) |                     # Group 1: URLs\n",
        "        (www\\.[^\\s]+) |                         # Group 2: www URLs\n",
        "        (\\w+@\\w+\\.\\w+) |                        # Group 3: Emails\n",
        "        (\\d{1,2}/\\d{1,2}/\\d{2,4}) |             # Group 4: Dates\n",
        "        (\\d+\\.\\d+) |                            # Group 5: Decimal numbers\n",
        "        ([\\u0900-\\u097F]+) |                    # Group 6: Hindi (Devanagari) words\n",
        "        ([a-zA-Z0-9_-]+) |                      # Group 7: Latin words/digits\n",
        "        ([^\\s])                                 # Group 8: Other single chars\n",
        "    '''\n",
        "    tokens = re.findall(pattern, sentence, re.VERBOSE)\n",
        "    flat_tokens = [token for group in tokens for token in group if token]\n",
        "    return flat_tokens\n",
        "\n",
        "# -------------------- Dataset Processing and Saving --------------------\n",
        "def process_and_save_parquet(hindi_dataset, output_file='tokenized_data.parquet'):\n",
        "    all_data = []\n",
        "\n",
        "    for example in hindi_dataset:\n",
        "        if 'text' not in example or not example['text'].strip():\n",
        "            continue\n",
        "\n",
        "        text = example['text'].strip()\n",
        "        sentences = sentence_split(text)\n",
        "\n",
        "        for sentence in sentences:\n",
        "            word_tokens = word_tokenize(sentence)\n",
        "            all_data.append({\n",
        "                'original_text': text,\n",
        "                'sentence': sentence,\n",
        "                'tokens': word_tokens\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    # Save to compressed parquet file (e.g., snappy compression)\n",
        "    df.to_parquet(output_file, compression='snappy', index=False)\n",
        "    print(f\"✅ Saved {len(df)} tokenized sentences to '{output_file}'\")\n",
        "\n",
        "# -------------------- Example Run --------------------\n",
        "# Assuming hindi_dataset is already loaded as a list of dicts like: [{'text': '...'}, ...]\n",
        "# Call this function:\n",
        "# process_and_save_parquet(hindi_dataset)\n"
      ],
      "metadata": {
        "id": "DLoJgbP1pUvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# -------------------- Sentence Tokenizer --------------------\n",
        "def sentence_split(paragraph):\n",
        "    sentence_endings = ['.', '?', '!', '।']\n",
        "    sentences = []\n",
        "    current = ''\n",
        "    for char in paragraph:\n",
        "        current += char\n",
        "        if char in sentence_endings:\n",
        "            if current.strip():\n",
        "                sentences.append(current.strip())\n",
        "                current = ''\n",
        "    if current.strip():\n",
        "        sentences.append(current.strip())\n",
        "    return sentences\n",
        "\n",
        "# -------------------- Word Tokenizer --------------------\n",
        "def word_tokenize(sentence):\n",
        "    pattern = r'''\n",
        "        (https?://[^\\s]+) |\n",
        "        (www\\.[^\\s]+) |\n",
        "        (\\w+@\\w+\\.\\w+) |\n",
        "        (\\d{1,2}/\\d{1,2}/\\d{2,4}) |\n",
        "        (\\d+\\.\\d+) |\n",
        "        ([\\u0900-\\u097F]+) |\n",
        "        ([a-zA-Z0-9_-]+) |\n",
        "        ([^\\s])\n",
        "    '''\n",
        "    tokens = re.findall(pattern, sentence, re.VERBOSE)\n",
        "    flat_tokens = [token for group in tokens for token in group if token]\n",
        "    return flat_tokens\n",
        "\n",
        "# -------------------- Corpus Statistics Computation --------------------\n",
        "def compute_corpus_statistics(hindi_stream):\n",
        "    total_sentences = 0\n",
        "    total_words = 0\n",
        "    total_characters = 0\n",
        "    unique_tokens = set()\n",
        "\n",
        "    for example in hindi_stream:\n",
        "        if 'text' not in example or not example['text'].strip():\n",
        "            continue\n",
        "\n",
        "        text = example['text'].strip()\n",
        "        sentences = sentence_split(text)\n",
        "        total_sentences += len(sentences)\n",
        "\n",
        "        for sentence in sentences:\n",
        "            tokens = word_tokenize(sentence)\n",
        "            total_words += len(tokens)\n",
        "            for token in tokens:\n",
        "                total_characters += len(token)\n",
        "                unique_tokens.add(token)\n",
        "\n",
        "    # Derived statistics\n",
        "    avg_sentence_len = total_words / total_sentences if total_sentences else 0\n",
        "    avg_word_len = total_characters / total_words if total_words else 0\n",
        "    ttr = len(unique_tokens) / total_words if total_words else 0\n",
        "\n",
        "    print(\"\\n📊 Corpus Statistics:\")\n",
        "    print(f\"1. Total number of sentences: {total_sentences}\")\n",
        "    print(f\"2. Total number of words:     {total_words}\")\n",
        "    print(f\"3. Total number of characters:{total_characters}\")\n",
        "    print(f\"4. Avg sentence length:       {avg_sentence_len:.2f} words/sentence\")\n",
        "    print(f\"5. Avg word length:           {avg_word_len:.2f} characters/word\")\n",
        "    print(f\"6. Type/Token Ratio (TTR):    {ttr:.4f}\")\n",
        "\n",
        "# -------------------- Example Usage --------------------\n",
        "# compute_corpus_statistics(hindi_dataset)  # hindi_dataset is a stream\n"
      ],
      "metadata": {
        "id": "itEetP7Lkl6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from itertools import tee\n",
        "\n",
        "# Load the streaming dataset\n",
        "hindi_dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files=\"https://huggingface.co/datasets/ai4bharat/IndicCorpV2/resolve/main/data/hi-1.txt\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Duplicate the stream: one for saving, one for stats\n",
        "save_stream, stats_stream = tee(hindi_dataset)\n"
      ],
      "metadata": {
        "id": "OK6fNMiUk-22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm  # or from tqdm import tqdm\n",
        "\n",
        "# Wrap stream with tqdm\n",
        "from itertools import tee\n",
        "save_stream, stats_stream = tee(hindi_dataset)\n",
        "\n",
        "save_stream_tqdm = tqdm(save_stream, desc=\"⏳ Saving\", unit=\" lines\")\n",
        "\n",
        "# Now pass wrapped stream into function (no change inside it)\n",
        "process_and_save_parquet(save_stream_tqdm, output_file=\"tokenized_hindi.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "add3ecc0001947009bc14666bfbca8fa",
            "a912ca5563de4ca188b69a839e1c2ab2",
            "906465f81ad34ad48d043628590272a8",
            "16ca65eca2d34a1d81f59be7eeef37db",
            "a9769ab34a0c405992d037c750374bbd",
            "e1e32f740d404fdeb4c8dad3f0b1be72",
            "ef640178cc264a6cbd58a3d71eca0320",
            "94f1bdb5df944e62ae58b1508c422314",
            "604b4e6710274ba580f1aab19afc6837",
            "4a98a1df1c184b8f8e563e7bca97738d",
            "307ead61e6244cc7a556fcba97cd74b2"
          ]
        },
        "id": "blCxHzBfln63",
        "outputId": "c704413c-29e9-42de-e308-7f4db75a5d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "⏳ Saving: 0 lines [00:00, ? lines/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "add3ecc0001947009bc14666bfbca8fa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "import unicodedata\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Stream Hindi dataset from Hugging Face without full download\n",
        "hindi_dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files=\"https://huggingface.co/datasets/ai4bharat/IndicCorpV2/resolve/main/data/hi-1.txt\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# -------------------- Sentence Tokenizer --------------------\n",
        "def sentence_split(paragraph):\n",
        "    sentence_endings = ['.', '?', '!', '।']\n",
        "    sentences = []\n",
        "    current = ''\n",
        "    for char in paragraph:\n",
        "        current += char\n",
        "        if char in sentence_endings:\n",
        "            if current.strip():\n",
        "                sentences.append(current.strip())\n",
        "                current = ''\n",
        "    if current.strip():\n",
        "        sentences.append(current.strip())\n",
        "    return sentences\n",
        "\n",
        "# -------------------- Word Tokenizer using regex --------------------\n",
        "def word_tokenize(sentence):\n",
        "\n",
        "    pattern = r'''\n",
        "        (https?://[^\\s]+) |                     # Group 1: URLs\n",
        "        (www\\.[^\\s]+) |                         # Group 2: www URLs\n",
        "        (\\w+@\\w+\\.\\w+) |                        # Group 3: Emails\n",
        "        (\\d{1,2}/\\d{1,2}/\\d{2,4}) |             # Group 4: Dates\n",
        "        (\\d+\\.\\d+) |                            # Group 5: Decimal numbers\n",
        "        ([\\u0900-\\u097F]+) |                    # Group 6: Hindi (Devanagari) words\n",
        "        ([a-zA-Z0-9_-]+) |                      # Group 7: Latin words/digits\n",
        "        ([^\\s])                                 # Group 8: Other single chars\n",
        "    '''\n",
        "    tokens = re.findall(pattern, sentence, re.VERBOSE)\n",
        "    flat_tokens = [token for group in tokens for token in group if token]\n",
        "    return flat_tokens\n",
        "\n",
        "# -------------------- Corpus Statistics Class --------------------\n",
        "class CorpusStatistics:\n",
        "    def __init__(self):\n",
        "        self.total_sentences = 0\n",
        "        self.total_words = 0\n",
        "        self.total_characters = 0\n",
        "        self.sentence_lengths = []\n",
        "        self.word_lengths = []\n",
        "        self.vocabulary = Counter()\n",
        "        self.processed_documents = 0\n",
        "        self.tokenized_data = []\n",
        "\n",
        "    def process_document(self, text: str, doc_id: int) -> Dict[str, Any]:\n",
        "        \"\"\"Process a single document and update statistics\"\"\"\n",
        "        if not text or not text.strip():\n",
        "            return None\n",
        "\n",
        "        text = text.strip()\n",
        "        sentences = sentence_split(text)\n",
        "\n",
        "        processed_sentences = []\n",
        "        doc_word_count = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if not sentence.strip():\n",
        "                continue\n",
        "\n",
        "            words = word_tokenize(sentence)\n",
        "\n",
        "            if words:  # Only process non-empty sentences\n",
        "                processed_sentences.append({\n",
        "                    'text': sentence,\n",
        "                    'tokens': words,\n",
        "                    'word_count': len(words)\n",
        "                })\n",
        "\n",
        "                # Update statistics\n",
        "                self.total_sentences += 1\n",
        "                self.total_words += len(words)\n",
        "                self.total_characters += len(sentence)\n",
        "                self.sentence_lengths.append(len(words))\n",
        "\n",
        "                # Update vocabulary\n",
        "                self.vocabulary.update(words)\n",
        "\n",
        "                # Track word lengths\n",
        "                for word in words:\n",
        "                    self.word_lengths.append(len(word))\n",
        "\n",
        "                doc_word_count += len(words)\n",
        "\n",
        "        if processed_sentences:\n",
        "            self.processed_documents += 1\n",
        "\n",
        "            document_data = {\n",
        "                'document_id': doc_id,\n",
        "                'original_text': text,\n",
        "                'sentences': processed_sentences,\n",
        "                'document_stats': {\n",
        "                    'sentence_count': len(processed_sentences),\n",
        "                    'word_count': doc_word_count,\n",
        "                    'character_count': len(text)\n",
        "                }\n",
        "            }\n",
        "\n",
        "            return document_data\n",
        "\n",
        "        return None\n",
        "\n",
        "    def compute_final_statistics(self) -> Dict[str, float]:\n",
        "        \"\"\"Compute all required corpus statistics\"\"\"\n",
        "        if self.total_sentences == 0:\n",
        "            return {}\n",
        "\n",
        "        # Calculate averages\n",
        "        avg_sentence_length = (sum(self.sentence_lengths) / len(self.sentence_lengths)) if self.sentence_lengths else 0\n",
        "        avg_word_length = (sum(self.word_lengths) / len(self.word_lengths)) if self.word_lengths else 0\n",
        "\n",
        "        # Calculate Type-Token Ratio\n",
        "        unique_tokens = len(self.vocabulary)\n",
        "        total_tokens = self.total_words\n",
        "        ttr = unique_tokens / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'total_sentences': self.total_sentences,\n",
        "            'total_words': self.total_words,\n",
        "            'total_characters': self.total_characters,\n",
        "            'average_sentence_length': round(avg_sentence_length, 2),\n",
        "            'average_word_length': round(avg_word_length, 2),\n",
        "            'type_token_ratio': round(ttr, 4),\n",
        "            'vocabulary_size': unique_tokens,\n",
        "            'processed_documents': self.processed_documents\n",
        "        }\n",
        "\n",
        "    def save_data_and_statistics(self, output_dir: str = \"hindi_corpus_output\"):\n",
        "        \"\"\"Save tokenized data and statistics to files - optimized for large datasets\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # For very large datasets, save in chunks to manage memory\n",
        "        chunk_size = 10000\n",
        "        if len(self.tokenized_data) > chunk_size:\n",
        "            print(f\"Saving large dataset in chunks of {chunk_size} documents...\")\n",
        "\n",
        "            # Save in multiple JSON files\n",
        "            for i in range(0, len(self.tokenized_data), chunk_size):\n",
        "                chunk = self.tokenized_data[i:i+chunk_size]\n",
        "                chunk_file = os.path.join(output_dir, f\"tokenized_data_chunk_{i//chunk_size + 1:03d}.json\")\n",
        "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(chunk, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            # Save complete data in pickle format (more memory efficient)\n",
        "            pickle_file = os.path.join(output_dir, \"tokenized_data_complete.pkl\")\n",
        "            with open(pickle_file, 'wb') as f:\n",
        "                pickle.dump(self.tokenized_data, f)\n",
        "\n",
        "        else:\n",
        "            # Save tokenized data in JSON format\n",
        "            json_file = os.path.join(output_dir, \"tokenized_data.json\")\n",
        "            with open(json_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.tokenized_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            # Save tokenized data in pickle format (faster loading)\n",
        "            pickle_file = os.path.join(output_dir, \"tokenized_data.pkl\")\n",
        "            with open(pickle_file, 'wb') as f:\n",
        "                pickle.dump(self.tokenized_data, f)\n",
        "\n",
        "        # Compute and save statistics\n",
        "        stats = self.compute_final_statistics()\n",
        "\n",
        "        # Save statistics as JSON\n",
        "        stats_file = os.path.join(output_dir, \"corpus_statistics.json\")\n",
        "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save top vocabulary (to manage file size for large corpora)\n",
        "        vocab_file = os.path.join(output_dir, \"vocabulary_top10000.json\")\n",
        "        vocab_dict = dict(self.vocabulary.most_common(10000))  # Top 10K words\n",
        "        with open(vocab_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(vocab_dict, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save complete vocabulary in pickle format\n",
        "        full_vocab_file = os.path.join(output_dir, \"vocabulary_complete.pkl\")\n",
        "        with open(full_vocab_file, 'wb') as f:\n",
        "            pickle.dump(dict(self.vocabulary), f)\n",
        "\n",
        "        # Save detailed report\n",
        "        report_file = os.path.join(output_dir, \"statistics_report.txt\")\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"NLP ASSIGNMENT 1 - HINDI CORPUS STATISTICS REPORT (EXTENDED)\\n\")\n",
        "            f.write(\"=\" * 65 + \"\\n\\n\")\n",
        "            f.write(\"Dataset: ai4bharat/IndicCorpV2 (Hindi)\\n\")\n",
        "            f.write(\"File: hi-1.txt\\n\")\n",
        "            f.write(f\"Processing Scale: {stats['processed_documents']:,} documents\\n\\n\")\n",
        "\n",
        "            f.write(\"ASSIGNMENT REQUIREMENTS (Task 1d):\\n\")\n",
        "            f.write(\"-\" * 35 + \"\\n\")\n",
        "            f.write(f\"i.   Total number of sentences: {stats['total_sentences']:,}\\n\")\n",
        "            f.write(f\"ii.  Total number of words: {stats['total_words']:,}\\n\")\n",
        "            f.write(f\"iii. Total number of characters: {stats['total_characters']:,}\\n\")\n",
        "            f.write(f\"iv.  Average Sentence Length: {stats['average_sentence_length']} words per sentence\\n\")\n",
        "            f.write(f\"v.   Average word length: {stats['average_word_length']} characters per word\\n\")\n",
        "            f.write(f\"vi.  Type/Token Ratio (TTR): {stats['type_token_ratio']}\\n\\n\")\n",
        "\n",
        "            f.write(\"EXTENDED STATISTICS:\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "            f.write(f\"Vocabulary size (unique tokens): {stats['vocabulary_size']:,}\\n\")\n",
        "            f.write(f\"Processed documents: {stats['processed_documents']:,}\\n\")\n",
        "            f.write(f\"Longest sentence: {max(self.sentence_lengths) if self.sentence_lengths else 0} words\\n\")\n",
        "            f.write(f\"Shortest sentence: {min(self.sentence_lengths) if self.sentence_lengths else 0} words\\n\")\n",
        "            f.write(f\"Longest word: {max(self.word_lengths) if self.word_lengths else 0} characters\\n\\n\")\n",
        "\n",
        "            f.write(\"TOP 50 MOST FREQUENT TOKENS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            for i, (word, freq) in enumerate(self.vocabulary.most_common(50), 1):\n",
        "                f.write(f\"{i:2d}. {word}: {freq:,}\\n\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "# -------------------- Main Processing --------------------\n",
        "def main():\n",
        "    print(\"NLP ASSIGNMENT 1: Text Preprocessing with Hindi IndicCorpV2\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Dataset: ai4bharat/IndicCorpV2 (Hindi - hi-1.txt)\")\n",
        "    print(\"Streaming from: https://huggingface.co/datasets/ai4bharat/IndicCorpV2\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize corpus statistics\n",
        "    corpus_stats = CorpusStatistics()\n",
        "\n",
        "    print(\"\\nProcessing Hindi dataset...\\n\")\n",
        "\n",
        "    # Process first few examples for demonstration (like your original code)\n",
        "    print(\"SAMPLE PROCESSING (First 5 examples):\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    sample_count = 0\n",
        "    for i, example in enumerate(hindi_dataset):\n",
        "        # Skip if 'text' is missing or just whitespace\n",
        "        if 'text' not in example or not example['text'].strip():\n",
        "            continue\n",
        "\n",
        "        text = example['text'].strip()\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        # Show sample processing for first 5 examples\n",
        "        if sample_count < 5:\n",
        "            print(f\"\\n--- Example {sample_count + 1} ---\")\n",
        "            print(\"Original Paragraph:\")\n",
        "            print(text[:200] + \"...\" if len(text) > 200 else text)\n",
        "\n",
        "            sentences = sentence_split(text)\n",
        "\n",
        "            print(f\"\\nTokenized Sentences ({len(sentences)} total):\")\n",
        "            for j, sent in enumerate(sentences[:2]):  # Show first 2 sentences\n",
        "                print(f\"{j+1}. {sent}\")\n",
        "            if len(sentences) > 2:\n",
        "                print(f\"... and {len(sentences) - 2} more sentences\")\n",
        "\n",
        "            print(\"\\nTokenized Words (first sentence):\")\n",
        "            if sentences:\n",
        "                tokens = word_tokenize(sentences[0])\n",
        "                print(tokens[:15])  # Show first 15 tokens\n",
        "                if len(tokens) > 15:\n",
        "                    print(f\"... and {len(tokens) - 15} more tokens\")\n",
        "\n",
        "            sample_count += 1\n",
        "\n",
        "        # Process document for statistics\n",
        "        processed_doc = corpus_stats.process_document(text, i)\n",
        "        if processed_doc:\n",
        "            corpus_stats.tokenized_data.append(processed_doc)\n",
        "\n",
        "        # Progress indicator - more frequent updates for larger processing\n",
        "        if i > 0 and i % 5000 == 0:\n",
        "            print(f\"\\nProcessed {i:,} documents...\")\n",
        "            print(f\"Current stats: {corpus_stats.total_sentences:,} sentences, {corpus_stats.total_words:,} words\")\n",
        "            print(f\"Current TTR: {len(corpus_stats.vocabulary) / corpus_stats.total_words:.4f}\" if corpus_stats.total_words > 0 else \"\")\n",
        "\n",
        "            # Show memory-friendly batch saving every 25,000 documents\n",
        "            if i % 25000 == 0:\n",
        "                print(f\"Saving checkpoint at {i:,} documents...\")\n",
        "                temp_stats = corpus_stats.save_data_and_statistics(f\"hindi_corpus_checkpoint_{i}\")\n",
        "                print(f\"Checkpoint saved: {temp_stats['total_sentences']:,} sentences processed so far\")\n",
        "\n",
        "        # Process more documents for comprehensive statistics\n",
        "        # You can adjust this number or remove the condition entirely for full dataset\n",
        "        if i >= 100000:  # Process 100,000 documents (remove this line for full dataset)\n",
        "            print(f\"\\nProcessed {i:,} documents - stopping for memory management...\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\nProcessing complete!\")\n",
        "    print(f\"Processed {corpus_stats.processed_documents:,} documents\")\n",
        "\n",
        "    # Save data and compute statistics\n",
        "    print(\"\\nSaving tokenized data and computing statistics...\")\n",
        "    stats = corpus_stats.save_data_and_statistics()\n",
        "\n",
        "    # Display final statistics\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FINAL CORPUS STATISTICS (Assignment Task 1d)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"i.   Total number of sentences: {stats['total_sentences']:,}\")\n",
        "    print(f\"ii.  Total number of words: {stats['total_words']:,}\")\n",
        "    print(f\"iii. Total number of characters: {stats['total_characters']:,}\")\n",
        "    print(f\"iv.  Average Sentence Length: {stats['average_sentence_length']} words per sentence\")\n",
        "    print(f\"v.   Average word length: {stats['average_word_length']} characters per word\")\n",
        "    print(f\"vi.  Type/Token Ratio (TTR): {stats['type_token_ratio']}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Vocabulary size: {stats['vocabulary_size']:,} unique tokens\")\n",
        "    print(f\"Processed documents: {stats['processed_documents']:,}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(f\"\\n✅ ASSIGNMENT TASK 1 COMPLETED - EXTENDED VERSION!\")\n",
        "    print(f\"📊 SCALE: Processed {stats['processed_documents']:,} documents\")\n",
        "    print(f\"📁 Check 'hindi_corpus_output' directory for:\")\n",
        "    if stats['processed_documents'] > 10000:\n",
        "        print(f\"📁 tokenized_data_chunk_*.json - Chunked tokenized documents\")\n",
        "        print(f\"📁 tokenized_data_complete.pkl - Complete binary format\")\n",
        "        print(f\"📁 vocabulary_top10000.json - Top 10,000 frequent words\")\n",
        "        print(f\"📁 vocabulary_complete.pkl - Complete vocabulary\")\n",
        "    else:\n",
        "        print(f\"📁 tokenized_data.json - All tokenized documents\")\n",
        "        print(f\"📁 tokenized_data.pkl - Fast-loading binary format\")\n",
        "        print(f\"📁 vocabulary_top10000.json - Top frequent words\")\n",
        "    print(f\"📁 corpus_statistics.json - All computed statistics\")\n",
        "    print(f\"📁 statistics_report.txt - Extended human-readable report\")\n",
        "\n",
        "    print(f\"\\n🎯 All requirements completed at scale:\")\n",
        "    print(f\"✅ a. Downloaded and extracted {stats['processed_documents']:,} Hindi documents\")\n",
        "    print(f\"✅ b. Tokenized {stats['total_sentences']:,} sentences into {stats['total_words']:,} words\")\n",
        "    print(f\"✅ c. Saved all tokenized data with memory-efficient chunking\")\n",
        "    print(f\"✅ d. Computed comprehensive corpus statistics\")\n",
        "\n",
        "    print(f\"\\n CORPUS INSIGHTS:\")\n",
        "    print(f\" Vocabulary richness: {stats['vocabulary_size']:,} unique tokens\")\n",
        "    print(f\" Text density: {stats['total_characters']:,} characters processed\")\n",
        "    print(f\" Language diversity: TTR = {stats['type_token_ratio']} (lower = more repetitive)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ0ZmzVclrXf",
        "outputId": "bf5dbd86-9b26-4310-f176-e9676c84f7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ASSIGNMENT 1: Text Preprocessing with Hindi IndicCorpV2\n",
            "============================================================\n",
            "Dataset: ai4bharat/IndicCorpV2 (Hindi - hi-1.txt)\n",
            "Streaming from: https://huggingface.co/datasets/ai4bharat/IndicCorpV2\n",
            "============================================================\n",
            "\n",
            "Processing Hindi dataset...\n",
            "\n",
            "SAMPLE PROCESSING (First 5 examples):\n",
            "----------------------------------------\n",
            "\n",
            "--- Example 1 ---\n",
            "Original Paragraph:\n",
            "लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "Tokenized Sentences (1 total):\n",
            "1. लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "Tokenized Words (first sentence):\n",
            "['लोगों', 'को', 'बिलों', 'संबंधी', 'सुविधा', 'देना', 'ही', 'उनका', 'काम']\n",
            "\n",
            "--- Example 2 ---\n",
            "Original Paragraph:\n",
            "इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था। हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ क...\n",
            "\n",
            "Tokenized Sentences (7 total):\n",
            "1. इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था।\n",
            "2. हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी।\n",
            "... and 5 more sentences\n",
            "\n",
            "Tokenized Words (first sentence):\n",
            "['इनेलो', '1987', 'में', 'उस', 'वक्त', 'ऐसे', 'ही', 'दोराहे', 'पर', 'खड़ी', 'थी', ',', 'जब', 'पूर्व', 'उपप्रधानमंत्री']\n",
            "... and 13 more tokens\n",
            "\n",
            "--- Example 3 ---\n",
            "Original Paragraph:\n",
            "जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n",
            "\n",
            "Tokenized Sentences (1 total):\n",
            "1. जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n",
            "\n",
            "Tokenized Words (first sentence):\n",
            "['जहां', 'आई', 'थी', 'तबाही', 'उस', 'घाटी', 'क्षेत्र', 'में', 'खतरा', 'ज्यादा']\n",
            "\n",
            "--- Example 4 ---\n",
            "Original Paragraph:\n",
            "इसके बाद केंद्र की ओर से प्रदेश सरकार को पीएमजीएसवाई में 200 करोड़ रुपये की राशि उपलब्ध करा दी गई। भाजपा के मीडिया प्रभारी दिवाकर सिंह ने शनिवार को बताया कि केंद्र ने प्रदेश सरकार को 200 करोड़ रुपये भ...\n",
            "\n",
            "Tokenized Sentences (2 total):\n",
            "1. इसके बाद केंद्र की ओर से प्रदेश सरकार को पीएमजीएसवाई में 200 करोड़ रुपये की राशि उपलब्ध करा दी गई।\n",
            "2. भाजपा के मीडिया प्रभारी दिवाकर सिंह ने शनिवार को बताया कि केंद्र ने प्रदेश सरकार को 200 करोड़ रुपये भेजा है।\n",
            "\n",
            "Tokenized Words (first sentence):\n",
            "['इसके', 'बाद', 'केंद्र', 'की', 'ओर', 'से', 'प्रदेश', 'सरकार', 'को', 'पीएमजीएसवाई', 'में', '200', 'करोड़', 'रुपये', 'की']\n",
            "... and 5 more tokens\n",
            "\n",
            "--- Example 5 ---\n",
            "Original Paragraph:\n",
            "यह पूछने पर कि इस बड़े मैच से पहले उनकी नींद गायब हुई तो बाबर ने कहा, \"हम काफी टूर्नामेंट खेल चुके हैं, हमने चैम्पियंस ट्राफी में भी अच्छा किया था. हम इसे जितना सरल रखेंगे, उतना ही बेहतर होगा. इसमें स...\n",
            "\n",
            "Tokenized Sentences (6 total):\n",
            "1. यह पूछने पर कि इस बड़े मैच से पहले उनकी नींद गायब हुई तो बाबर ने कहा, \"हम काफी टूर्नामेंट खेल चुके हैं, हमने चैम्पियंस ट्राफी में भी अच्छा किया था.\n",
            "2. हम इसे जितना सरल रखेंगे, उतना ही बेहतर होगा.\n",
            "... and 4 more sentences\n",
            "\n",
            "Tokenized Words (first sentence):\n",
            "['यह', 'पूछने', 'पर', 'कि', 'इस', 'बड़े', 'मैच', 'से', 'पहले', 'उनकी', 'नींद', 'गायब', 'हुई', 'तो', 'बाबर']\n",
            "... and 20 more tokens\n",
            "\n",
            "Processed 5,000 documents...\n",
            "Current stats: 8,688 sentences, 149,979 words\n",
            "Current TTR: 0.1198\n",
            "\n",
            "Processed 10,000 documents...\n",
            "Current stats: 17,466 sentences, 304,856 words\n",
            "Current TTR: 0.0893\n",
            "\n",
            "Processed 15,000 documents...\n",
            "Current stats: 26,652 sentences, 466,054 words\n",
            "Current TTR: 0.0752\n",
            "\n",
            "Processed 20,000 documents...\n",
            "Current stats: 35,399 sentences, 617,723 words\n",
            "Current TTR: 0.0664\n",
            "\n",
            "Processed 25,000 documents...\n",
            "Current stats: 44,176 sentences, 771,322 words\n",
            "Current TTR: 0.0604\n",
            "Saving checkpoint at 25,000 documents...\n",
            "Saving large dataset in chunks of 10000 documents...\n",
            "Checkpoint saved: 44,176 sentences processed so far\n",
            "\n",
            "Processed 30,000 documents...\n",
            "Current stats: 53,013 sentences, 923,722 words\n",
            "Current TTR: 0.0557\n",
            "\n",
            "Processed 35,000 documents...\n",
            "Current stats: 61,795 sentences, 1,077,612 words\n",
            "Current TTR: 0.0521\n",
            "\n",
            "Processed 40,000 documents...\n",
            "Current stats: 70,596 sentences, 1,232,808 words\n",
            "Current TTR: 0.0492\n",
            "\n",
            "Processed 45,000 documents...\n",
            "Current stats: 79,111 sentences, 1,379,957 words\n",
            "Current TTR: 0.0470\n",
            "\n",
            "Processed 50,000 documents...\n",
            "Current stats: 87,557 sentences, 1,527,390 words\n",
            "Current TTR: 0.0448\n",
            "Saving checkpoint at 50,000 documents...\n",
            "Saving large dataset in chunks of 10000 documents...\n",
            "Checkpoint saved: 87,557 sentences processed so far\n",
            "\n",
            "Processed 55,000 documents...\n",
            "Current stats: 96,112 sentences, 1,676,672 words\n",
            "Current TTR: 0.0429\n",
            "\n",
            "Processed 60,000 documents...\n",
            "Current stats: 105,510 sentences, 1,844,378 words\n",
            "Current TTR: 0.0411\n",
            "\n",
            "Processed 65,000 documents...\n",
            "Current stats: 114,523 sentences, 2,007,109 words\n",
            "Current TTR: 0.0398\n",
            "\n",
            "Processed 70,000 documents...\n",
            "Current stats: 123,771 sentences, 2,171,661 words\n",
            "Current TTR: 0.0385\n",
            "\n",
            "Processed 75,000 documents...\n",
            "Current stats: 132,733 sentences, 2,326,772 words\n",
            "Current TTR: 0.0376\n",
            "Saving checkpoint at 75,000 documents...\n",
            "Saving large dataset in chunks of 10000 documents...\n",
            "Checkpoint saved: 132,733 sentences processed so far\n",
            "\n",
            "Processed 80,000 documents...\n",
            "Current stats: 141,561 sentences, 2,480,205 words\n",
            "Current TTR: 0.0366\n",
            "\n",
            "Processed 85,000 documents...\n",
            "Current stats: 149,854 sentences, 2,623,937 words\n",
            "Current TTR: 0.0357\n",
            "\n",
            "Processed 90,000 documents...\n",
            "Current stats: 158,799 sentences, 2,778,075 words\n",
            "Current TTR: 0.0348\n",
            "\n",
            "Processed 95,000 documents...\n",
            "Current stats: 167,383 sentences, 2,926,773 words\n",
            "Current TTR: 0.0340\n",
            "\n",
            "Processed 100,000 documents...\n",
            "Current stats: 175,683 sentences, 3,071,468 words\n",
            "Current TTR: 0.0333\n",
            "Saving checkpoint at 100,000 documents...\n",
            "Saving large dataset in chunks of 10000 documents...\n",
            "Checkpoint saved: 175,683 sentences processed so far\n",
            "\n",
            "Processed 100,000 documents - stopping for memory management...\n",
            "\n",
            "Processing complete!\n",
            "Processed 50,001 documents\n",
            "\n",
            "Saving tokenized data and computing statistics...\n",
            "Saving large dataset in chunks of 10000 documents...\n",
            "\n",
            "============================================================\n",
            "FINAL CORPUS STATISTICS (Assignment Task 1d)\n",
            "============================================================\n",
            "i.   Total number of sentences: 175,683\n",
            "ii.  Total number of words: 3,071,468\n",
            "iii. Total number of characters: 14,526,607\n",
            "iv.  Average Sentence Length: 17.48 words per sentence\n",
            "v.   Average word length: 3.84 characters per word\n",
            "vi.  Type/Token Ratio (TTR): 0.0333\n",
            "------------------------------------------------------------\n",
            "Vocabulary size: 102,326 unique tokens\n",
            "Processed documents: 50,001\n",
            "============================================================\n",
            "\n",
            "✅ ASSIGNMENT TASK 1 COMPLETED - EXTENDED VERSION!\n",
            "📊 SCALE: Processed 50,001 documents\n",
            "📁 Check 'hindi_corpus_output' directory for:\n",
            "📁 tokenized_data_chunk_*.json - Chunked tokenized documents\n",
            "📁 tokenized_data_complete.pkl - Complete binary format\n",
            "📁 vocabulary_top10000.json - Top 10,000 frequent words\n",
            "📁 vocabulary_complete.pkl - Complete vocabulary\n",
            "📁 corpus_statistics.json - All computed statistics\n",
            "📁 statistics_report.txt - Extended human-readable report\n",
            "\n",
            "🎯 All requirements completed at scale:\n",
            "✅ a. Downloaded and extracted 50,001 Hindi documents\n",
            "✅ b. Tokenized 175,683 sentences into 3,071,468 words\n",
            "✅ c. Saved all tokenized data with memory-efficient chunking\n",
            "✅ d. Computed comprehensive corpus statistics\n",
            "\n",
            "📈 CORPUS INSIGHTS:\n",
            "🔤 Vocabulary richness: 102,326 unique tokens\n",
            "📝 Text density: 14,526,607 characters processed\n",
            "🎭 Language diversity: TTR = 0.0333 (lower = more repetitive)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "33CNkj-Mpbby"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}